# LLM Benchmarks for Data Analysis and Math

There are several public benchmarks for LLMs focused on data analysis and advanced math tasks. These benchmarks evaluate LLMs on reasoning, statistical operations, and structured data processing.

## Benchmarks for Data Analysis

These evaluate LLMs on structured data manipulation, SQL queries, and statistical analysis.

| Benchmark | Focus Area | Dataset Example | Used By |
|-----------|------------|-----------------|---------|
| BIRD (Benchmark for Inductive Reasoning in Data) | Data querying, aggregation, trend analysis | Tabular datasets | OpenAI, Anthropic |
| Spider | Complex SQL execution on natural language questions | SQL query execution | OpenAI, Meta, Hugging Face |
| TabFact | Fact verification on structured data | Wikipedia tables | Google DeepMind |
| HackerNews SQL Benchmark | SQL retrieval from unstructured text | HackerNews posts & metadata | OpenAI Codex, GPT-4 |
| FinQA | Financial data analysis, reasoning over tables | SEC filings, earnings reports | AI2, Bloomberg |

**Use Case for QuantaLogic:** If we test its retrieval and execution on SQL-based benchmarks, we can measure how well it processes real-world tabular data.

## Benchmarks for Advanced Math

These focus on symbolic reasoning, algebra, calculus, and theorem proving.

| Benchmark | Focus Area | Dataset Example | Used By |
|-----------|------------|-----------------|---------|
| MATH (A Benchmark for Math Word Problems) | Algebra, calculus, geometry, statistics | High school math problems | OpenAI, DeepMind |
| GSM8K (Grade School Math 8K) | Arithmetic & problem-solving | Word problems | GPT-4, Claude, Mistral |
| AMPS (Advanced Math Problem Solving) | Olympiad-level math | International Olympiad Problems | Google DeepMind |
| MiniF2F | Mathematical proofs | Theorem-solving | Meta, OpenAI |
| HELM-Math | Higher education-level math | University-level problem sets | AI research labs |